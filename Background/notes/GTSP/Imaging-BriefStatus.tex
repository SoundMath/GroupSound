%%% Previously we used cyclic group notation for these
%%% indexing groups but I don't see any reason to emphasize
%%% the cyclic property here.
%%%\def\groupA{\ensuremath{\Z_N^2}}
   \newcommand\groupA{\ensuremath{A}}
%%% same for K:
%%%\def\groupK{\ensuremath{\mathbb{Z}/K}}
   \newcommand\groupK{\ensuremath{K}}
%%% same for J:
%%%\def\groupJ{\ensuremath{\mathbb{Z}/J}}
   \newcommand\groupJ{\ensuremath{J}}
%%%
\section{Poisson/Extended Object}\label{sec:poiss-object}
\author{Paul Billings and William DeMeo}
This section is based on Paul Billing's notes~\cite{Billings:2001}, but the
present exposition is set in the algebraic framework of the foregoing chapters.
%Throughout this section, let $A = \Z/N_1 \times \Z/N_2$. 

Define
\begin{alignat*}{2}
f &= \text{ object } \qquad  &&s= \text{ incoherent psf }\\
g &= \text{ noiseless image } \qquad  &&d = \text{ detected image }
\end{alignat*}
%where $x\in \Z_N^2$.
and assume $f, s, g, d \in \LA$.
By an image ``ensemble'' we mean a set of $K$ images, or frames,
indexed by 
$k \in \groupK = \{0, 1,2, \ldots,K-1\}$.
For the $k^{th}$ frame in the ensemble, suppose\footnote{We previously used $g_k =t\cdot C(f)s_k$,
where $t\in \LA$ is a truncation window, and $\cdot$ denotes pointwise
multiplication. 
However, since the function $t$ does not
influence any of the derivations, it is notationally cleaner to postpone
inclusion of the truncation factor.}
%\[g_k = t\cdot (f * s_k), \qquad k \in \groupK\]
\begin{equation}\label{eq:g}
g_k = C(f)s_k %\qquad k \in \groupK
\end{equation}

In (\ref{eq:g}) $C(f)s_k$ denotes convolution by $f$ of $s_k \in
\LA$. 
More explicitly, for any point $x = (x_1,x_2)$ in the image plane, we have
\[
g_k(x) = \sum_{y_1\in \Z/N_1}\sum_{y_2\in \Z/N_2} f(y_1,y_2)s_k(x_1-y_1,x_2-y_2),
\qquad k \in \groupK
\]

Recall the Poisson distribution with mean $\lambda$
has a probability mass function (pmf) given by:
\[
p(x|\lambda) = \frac{\lambda^x \E^{-\lambda}}{x!}
\]
%Suppose the detected image data, $d_k$, satisfy 
%\[d_k(x) = g_k(x) + n_k(x), \qquad k \in \Z_K, \; x\in \Z_N^2\]
%and further s
Suppose that, at each point $x\in \groupA$, the value $d_k(x)$ is a
realization of a Poisson process with mean $g_k(x)$.  Then 
\[
p\left(d_k(x)|g_k(x)\right) = \frac{g_k(x)^{d_k(x)} \E^{-g_k(x)}}{d_k(x)!}
\]

Denote the set of all pixels in images belonging to the detected ensemble by
\[
\vec{d} = \left\{d_k(x)\right\}_{\stackrel{k\in \groupK}{_{x\in \groupA}}}
\]
Similarly, denote the set of all pixels in images from the noiseless ensemble
by
\[
\vec{g} = \left\{g_k(x)\right\}_{\stackrel{k\in \groupK}{_{x\in \groupA}}}
\]
Assuming independence, the joint pmf for pixels in the detected ensemble is
%\begin{align}
\begin{equation}\label{eq:jpmf}
p\left(\vec{d}| \vec{g} \right) %&
= \prod_{k}\prod_{x} p\left(d_k(x)|g_k(x)\right) %\nonumber\\&
= \prod_{k}\prod_{x} \frac{g_k(x)^{d_k(x)}\; \E^{-g_k(x)}}{d_k(x)!}
%= \prod_{k\in \Z_K}\prod_{x\in \groupA} p\left(d_k(x)|g_k(x)\right) %\nonumber\\&
%= \prod_{k\in \Z_K}\prod_{x\in \groupA} \frac{g_k(x)^{d_k(x)}\; \E^{-g_k(x)}}{d_k(x)!}
\end{equation}%\end{align}
Here -- and throughout unless otherwise noted -- $k$ ranges over $\groupK$ and 
$x$ ranges over $\groupA$.

Given that we observed $\vec{d}$, we want to find the values $\vec{g}$
which yield a joint density, $p\left(\cdot| \vec{g} \right)$, from which the observed
data set, $\vec{d} $, is the most likely outcome.  In other words, we seek a
maximum likelihood estimate (MLE) of $\vec{g}$. 

Given the data $\vec{d} $, we maximize the right hand side 
of~(\ref{eq:jpmf}) over all values of $\vec{g} $. Therefore, it makes sense to 
write~(\ref{eq:jpmf}) as a function of $\vec{g} $ given $\vec{d} $:
\begin{equation*}%\label{eq:likelihood}
L\left(\vec{g} |\vec{d}  \right) 
= \prod_{k}\prod_{x} \frac{g_k(x)^{d_k(x)}\; \E^{-g_k(x)}}{d_k(x)!}
%= \prod_{k=1}^K \prod_{i=1}^{N^2} \frac{g_k(x_i)^{d_k(x_i)}\; \E^{-g_k(x_i)}}{d_k(x_i)!}
\end{equation*}
This is typically called the likelihood function. 
It's easier, and equivalent, to maximize the natural log
of the likelihood function, which is:
\begin{equation}\label{eq:likelihood}
\ell\left(\vec{g} |\vec{d}  \right) 
%= \sum_{k\in\Z_K} \sum_{x\in \groupA} d_k(x)\log g_k(x) - g_k(x) - \log
= \sum_{k=0}^{K-1} \sum_{x} d_k(x)\log g_k(x) - g_k(x) - \log
d_k(x)!
\end{equation}
The last term on the right hand side is constant, so it can be ignored for the
purposes of maximizing the so called ``log likelihood function,'' $\ell$.

Recall from~(\ref{eq:g}), $g_k$ is defined as $C(f)s_k$.  Here, $t$ is
known and we are trying to estimate $f$ and $s_k$.  As a 
consequence, we arrive at an estimate of $g_k$.  Thus,
instead of maximizing the likelihood over values of $g_k$, we maximize over
values of $f$ and $s_k$ simultaneously. 

%\subsection{Gradient with respect to $g$}
%Ultimately, we want to find the parameter set $\vec{g}$ which is most likely to have produced
%the data set $\vec{d}$, in the MLE sense described above.  Thus, for each 
%$k'\in \Z_K$ and $x'\in \groupA$, we maximize the 
%likelihood function~(\ref{eq:likelihood}) by taking its gradient with respect to
%$g_{k'}(x')$ and finding that value of $g_{k'}(x')$ for which this gradient is 0.
%\begin{align*}
%\frac{\partial \ell\left(\vec{g} |\vec{d}\right) }{\partial g_{k'}(x')}   
%&= \frac{\partial}{\partial g_{k'}(x')} \sum_{k} \sum_{x} d_k(x)\log g_k(x) - g_k(x) - \log d_k(x)!\\
%&= \frac{\partial}{\partial g_{k'}(x')} \left[ d_{k'}(x')\log g_{k'}(x') - g_{k'}(x')\right]\\
%&= \frac{ d_{k'}(x)}{g_{k'}(x)} - 1
%\end{align*}
%As noted above, maximizing the likelihood over the parameter set $\vec{g}$ is equivalent to
%maximizing over $f$ and $s$ simultaneously.  The following sections describe
%this correspondence by deriving the gradients necessary for simultaneously
%maximizing $\ell$ with respect to $f$ and $s$.

\subsection{Gradient with respect to $f$}
Maximizing the log likelihood function over possible values of the
object leads to the MLE of the object, which we denote by $\tilde{f}$.
To derive this estimate we find that value of $f$ at which the derivative,
%of~(\ref{eq:likelihood}), 
with respect to $f(z)$, equals 0 for each $z \in \groupA$.
\begin{align*}
\frac{\partial \ell\left(\vec{g} |\vec{d}\right)}{\partial f(z)} 
&= \sum_{k=0}^{K-1} \sum_{x\in A} \frac{\partial}{\partial f(z)}\left[ d_k(x)\log g_k(x) - g_k(x)\right]\\
&= \sum_{k=0}^{K-1} \sum_{x\in A} \frac{\partial}{\partial g_k(x)}\left[ d_k(x)\log g_k(x) - g_k(x)\right] \frac{\partial g_k(x)}{\partial f(z)}\\
&= \sum_{k=0}^{K-1} \sum_{x\in A} \left[ \frac{ d_k(x)}{g_k(x)} - 1\right]\frac{\partial g_k(x)}{\partial f(z)} 
\end{align*}
and
\[%\begin{align*}
\frac{\partial g_k}{\partial f(z)} = \frac{\partial}{\partial f(z)} \;\sum_{y \in \groupA} f(y)\T(y)s_k
= \T(z)s_k\]
%\end{align*}
Therefore, 
\begin{equation}\label{eq:m1}
\frac{\partial \ell\left(\vec{g} |\vec{d}\right)}{\partial f(z)} 
= \sum_{k=0}^{K-1} \sum_{x\in A} \left[ \frac{ d_k(x)}{g_k(x)} - 1\right] \T(z)s_k(x)
\end{equation}

In simple optimization problems, the gradient is a
function of the variable over which we optimize.  In the present
case, the gradient is a function of $g_k$, which in turn is a function of $f$ via
equation~(\ref{eq:g}).  Therefore, given a set of $K$ psf
estimates $\{s_k\}_{0\leq k < K}$, we seek a vector $\tilde{f}$, %\,x\in \groupA$,
which, through~(\ref{eq:g}), minimizes the magnitude of~(\ref{eq:m1}) for each
$z \in \groupA$.  That is, we must find the vector $\tilde{f}$ which minimizes
\[
\left|
\sum_{k=0}^{K-1} \sum_{x\in A} \left[ \frac{ d_k(x)}{g_k(x)} - 1\right]\T(z) s_k(x)
\right|, \qquad z \in \groupA
\]

\subsection{Gradient with respect to $s_k$}
In order to maximize the likelihood function with respect to the PSF, $s_k$, we
could proceed by estimating $s_k$ directly, but it is often useful to 
write $s_k$ as a function of phase-aberration, and then estimate the form of
this function.
%of Zernike coefficients and estimate these coefficients.  
\subsubsection{PSF Phase Parameterization}
There are unknown phase errors across the aperture that distort the image.
We represent this distortion with a \emph{phase-aberration function},
$\Phi_k$, which may vary with $k$; in other words, the phase-aberration may be
different for each frame. 

In deriving the MLE of $s_k$ using the second approach mentioned
above, we first expand the phase-aberration function in a suitable basis.
(Often a \emph{Zernike basis} is used.)  
If we denote the set of basis functions by $\{\varphi_j\}_{0\leq j < J}$, then the
phase-aberration for the $k^{th}$ frame is expanded as follows:
\[
%\varphi(u) = \sum_{j=1}^J \alpha_j \varphi_j(u) 
\Phi_k = \sum_{j=0}^{J-1} \alpha_{kj} \varphi_j
\]
We usually take the basis functions $\varphi_j$ as given and not depending on
$k$.  On the other hand, the coefficients $\alpha_{kj}$ must be
estimated for each $k \in \groupK$.

\begin{remark} If the choice of basis was physically motivated,
it can help our intuition to think of $\alpha_{kj}$ as the 
projection of $\Phi_k$ onto the basis function $\varphi_j$. In fact, when
$\{\varphi_j\}_{0\leq j < J}$ is an orthonormal basis, the coefficients really are
projections.  In that case, we often write the expansion as follows:
\[ 
\Phi_k = \sum_{j=0}^{J-1} \langle \Phi_k, \varphi_j\rangle \varphi_j 
\]
\end{remark}

The incoherent PSF for the $k^{th}$ frame is
\begin{equation}\label{eq:s}
%s_k(x) = |h_k(x)|^2 %, \qquad x\in \groupA
s_k = |h_k|^2 = h_k h_k^* %, \qquad x\in \groupA
\end{equation}  
where $h_k$ is the coherent PSF, which is the inverse Fourier transform of the
coherent transfer function, or \emph{pupil function}.  

Above we defined a character $\tau_a: A \to U_N$ by the mapping
\[
\tau_a(x) = \E^{\I 2\pi \frac{x_1a_1}{N_1}}\E^{\I 2\pi \frac{x_2a_2}{N_2}}, \qquad
x \in \Z/N_1 \times \Z/N_2
\]
%\[\langle x,u\rangle = \E^{ i2\pi \frac{ux}{N_1N_2}}, \qquad x \in \groupA, u \in \groupA, \]
Using this notation, we state some standard Fourier transform relations on
which our model depends.
\begin{equation}\label{eq:h}
h_k = \frac{1}{N_1N_2} \invFT H_k = \frac{1}{N_1N_2} \sum_{u\in \groupA^*} H_k(u)\tau_u
\end{equation}  
The index set of the summation in (\ref{eq:h}) is $A^*$, the group of
characters of $A$. However, as noted above, we identify the
elements of $A^*$ with those of $A$ by the standard presentation, and simply
write the character group as $\groupA^* = \Z/N_1 \times \Z/N_2$.    

At any $x \in \groupA$, (\ref{eq:h}) evaluates to 
\[
h_k(x) = 
\frac{1}{N_1N_2} \sum_{u\in \groupA^*} H_k(u)\,
\E^{\I 2\pi \frac{x_1u_1}{N_1}}\E^{\I 2\pi \frac{x_2u_2}{N_2}}
\]

The transfer function is the Fourier transform of the point spread function.
\begin{equation}\label{eq:Hbasic}
H_k = \FT h_k = \sum_{x\in \groupA} h_k(x)\tau_x^*
\end{equation}
At any $u \in \groupA^*$, (\ref{eq:Hbasic}) evaluates to
\[
H_k(u) = \sum_{x \in \groupA} h_k(x)\,
\E^{-\I 2\pi \frac{x_1u_1}{N_1}}\E^{-\I 2\pi \frac{x_2u_2}{N_2}}
\]

Again, the summations are over the group $A = \Z/N_1 \times \Z/N_2$ and its
character group, $A^*$.  By isomorphism, the character group is the same index set,
$A^* = \Z/N_1 \times \Z/N_2$. 

The pupil function is complex-valued, and we take as its complex argument the
phase-aberration function.  That is, 
\begin{equation}\label{eq:H}
H_k(u) = |H_k(u)|\,\E^{\I \Phi_k(u)} = |H_k(u)|\,\E^{\I \sum_j \alpha_{kj}\varphi_j(u)}, \qquad u\in \groupA
\end{equation}

From equations (\ref{eq:s})--(\ref{eq:H}) we can write $s_k$ as a function of $\alpha_{kj}$.
%\[s_k(x) = |h_k(x)|^2  = h_k(x)\overline{h_k(x)}\]From (\ref{eq:h}),
\begin{align}\label{eq:sk}
s_k &=  h_k h_k^* \nonumber\\
&= \left(\frac{1}{N_1N_2}\sum_{u\in A^*} H_k(u) \tau_u\right)
\left( \frac{1}{N_1N_2}\sum_{v\in A^*}H_k^*(v)\tau_v^*\right) \nonumber\\
&=\frac{1}{(N_1N_2)^2}\sum_{u\in A^*} \sum_{v\in A^*} H_k(u)H_k^*(v)\tau_{u}\tau_{-v}
\end{align}

\begin{center}\setlength{\fboxsep}{4mm}\begin{boxedminipage}[t]{16cm}
\begin{remark} 
{\small At this point, we diverge slightly to make an observation which
simplifies equation~(\ref{eq:sk}).  Such a simplification may
only hold when working in the \emph{group algebra} $\C(\Z/N_1 \times \Z/N_2)$
(see \cite{Tolimieri:2003}).  Therefore, we make this point to indicate the
potential benefits of switching to the $\C(\Z/N_1 \times \Z/N_2)$ framework
for future analysis.  Thereafter, we resume without benefit of this
simplifying assumption.

Assume the following character formula:
\begin{equation}\label{eq:charprod}
\tau_u\tau_v = \left\{ \begin{array}{ll} o(A)\tau_u, &\quad \tau_v = \tau_u\\
                                         0, &\quad \text{ otherwise }
                       \end{array}\right.
\end{equation}

Expression (\ref{eq:charprod}) holds for $\tau_u, \tau_v \in \C A$.
Under this assumption, (\ref{eq:sk}) simplifies to
\[
s_k = \frac{1}{N_1N_2}\sum_{u\in A^*} H_k(u)H_k^*(-u)\tau_{u}
\]}
\end{remark}
\end{boxedminipage}\end{center}

%Since $\overline{H_k(u)} = |H_k(u)|\exp\{-\I \varphi_k(u)\}$, we have
Returning to expression (\ref{eq:sk}), since $H_k^*(u) =
|H_k(u)|\,\E^{-\I \varphi_k(u)}$, we have 
\begin{equation}\label{eq:m7}
s_k = 
\frac{1}{(N_1N_2)^2} \sum_{u\in A^*} \sum_{v\in A^*} |H_k(u)||H_k(v)|
\,\E^{ i\sum_j \alpha_{kj}[\varphi_j(u)-\varphi_j(v)]}\tau_{u-v}
\end{equation}
%with $u$ and $v$ each ranging over $A^* = \Z/N_1 \times \Z/N_2$.

%To derive the MLE of $s_k$, we first find ML estimates of $\alpha_{kj}$, and
%then work back through equations (\ref{eq:s})--(\ref{eq:H}) to derive corresponding estimate of $s_k$.  
Now that we have written $s_k$ as a function of $\{\alpha_{kj}\}_{0\leq j < J}$, we can
maximize the likelihood function~(\ref{eq:likelihood}) with respect to
$\alpha_{kj}$. For each index pair $(a,b)$, we have
\begin{align*}
\frac{\partial \ell\left(\vec{g} |\vec{d}\right)}{\partial \alpha_{ab}}  
&= \sum_{k=0}^{K-1} \sum_{x\in A} \left[ \frac{ d_k(x)}{g_k(x)} - 1\right] \frac{\partial g_k(x)}{\partial \alpha_{ab}} 
\end{align*}
and
\begin{equation}\label{eq:m4}
\frac{\partial g_k}{\partial \alpha_{ab}}= \sum_{y\in A} \frac{\partial g_k}{\partial s_k(y)}
\frac{\partial s_k(y)}{\partial \alpha_{ab}} 
\end{equation}
By the definition of $g_k$ and commutativity of convolution,
%\[ g_k(x) = t(x) \sum_{x'} f(x')s_k(x - x') \]
\[ g_k = C(f)s_k =C(s_k)f = \sum_{x\in A} s_k(x)\T(x)f \]
Therefore,
\begin{equation}\label{eq:m5}
\frac{\partial g_k}{\partial s_k(y)} = \T(y)f
\end{equation}

It remains only to compute the partial of $s_k$ with respect
$\alpha_{ab}$. From (\ref{eq:m7}),
\[
\frac{\partial s_k}{\partial \alpha_{ab}}  = 
\frac{1}{(N_1N_2)^2}\sum_{u\in A^*} \sum_{v\in A^*}  |H_k(u)||H_k(v)|\,i[\varphi_b(u)-\varphi_b(v)]\delta(k-a)
\,\E^{ i\sum_j \alpha_{kj}[\varphi_j(u)-\varphi_j(v)]} \tau_{u-v}
\]

Over the domain $(u,v) \in A^* \times A^*$, define %\footnote{The $i\pi/2$ term in the exponent accounts for $i = \E^{\I \pi/2}$.}
\[
\mathcal{H}_{ab}(u,v) = \frac{1}{(N_1N_2)^2}|H_a(u)||H_a(v)|\,[\varphi_b(u)-\varphi_b(v)]
\,\E^{ i \sum_j \alpha_{aj}[\varphi_j(u)-\varphi_j(v)] + i\frac{\pi}{2}}
\]
%--%Over the domain $u \in A^*$, define %\footnote{The $i\pi/2$ term in the exponent accounts for $i = \E^{\I \pi/2}$.}
%--%\[
%--%\mathcal{H}_{ab}(u,v) = \frac{1}{N_1N_2}|H_a(u)||H_a(-u)|\,[\varphi_b(u)-\varphi_b(-u)]
%--%\,\E^{ i \sum_j \alpha_{aj}[\varphi_j(u)-\varphi_j(-u)] + i\frac{\pi}{2}}
%--%\]
Then,
\begin{equation}\label{eq:m3}
\frac{\partial s_a}{\partial \alpha_{ab}}  = 
\sum_{u\in A^*} \sum_{v\in A^*} \mathcal{H}_{ab}(u,v) \tau_{u-v}, \qquad
\text{ and } \qquad \frac{\partial s_k}{\partial \alpha_{ab}}  = 0, \quad k \neq a
\end{equation}
Inserting equations~(\ref{eq:m5}) and~(\ref{eq:m3}) into equation~(\ref{eq:m4}) yields
\begin{align*}
\frac{\partial g_a}{\partial \alpha_{ab}}&= \sum_{y\in A} \sum_{u\in A^*} \sum_{v\in A^*} \mathcal{H}_{ab}(u,v) \tau_{u-v}(y)\T(y)f\\
%&= \sum_u \sum_v t(x)\mathcal{H}_{ab}(u,v) \sum_y f(x-y) \tau_{u-v}(y)\\
&= \sum_{u\in A^*} \sum_{v\in A^*} \mathcal{H}_{ab}(u,v) C(\tau_{u-v})f\\
&= \sum_{u\in A^*} \sum_{v\in A^*} \mathcal{H}_{ab}(u,v) \hat{f}(u-v)\tau_{u-v}
\end{align*}
where
\[
 \hat{f}(u) = \sum_{y\in A} f(y)\tau_u(y)
\]
denotes the Fourier coefficient of $f$ at $u$.

We can now formally express the gradient of $\ell$ with respect to $\alpha_{ab}$ as follows:
\begin{equation}\label{eq:m6}
\frac{\partial \ell\left(\vec{g} |\vec{d}\right)}{\partial \alpha_{ab}}  
= \sum_{x\in A}\sum_{u\in A^*}\sum_{v\in A^*} \left[ \frac{ d_a(x)}{g_a(x)} - 1\right]\mathcal{H}_{ab}(u,v) \hat{f}(u-v)\tau_{u-v}(x)
\end{equation}
If we let $r_a = \frac{d_a}{g_a} - 1$,
another Fourier coefficient in expression (\ref{eq:m6}) becomes apparent.
\[
 \hat{r}_a(u-v) = \sum_{x\in A} r_a(x)\tau_{u-v}(x)
\]
from which 
\begin{equation}\label{eq:m60}
\frac{\partial \ell\left(\vec{g} |\vec{d}\right)}{\partial \alpha_{ab}}  
= \sum_{u\in A^*}\sum_{v\in A^*} \mathcal{H}_{ab}(u,v) \hat{f}(u-v)\hat{r}_a(u-v)
\end{equation}

Equation~(\ref{eq:m60}) provides a formal expression of the gradient.
However, we require an expression which better facilitates algorithmic implementation.
Note that 
\[
s_a = h_a h_a^* = \Real[h_a]^2 + \Imag[h_a]^2
\]
is real valued.  Therefore, we can write equation~(\ref{eq:m7}) as
\[
s_k(x) = \Real [s_k(x)] = \frac{1}{(N_1N_2)^2} \sum_{u,v} |H_k(u)||H_k(v)|
\,\cos\left(\sum_j \alpha_{kj}[\varphi_j(u)-\varphi_j(v)] +
  2\pi\frac{(u_1-v_1)x_1}{N_1}+ 2\pi\frac{(u_2-v_2)x_2}{N_2}\right)
\]
from which, we have
\begin{equation}\label{eq:m8}
\frac{\partial s_a(y)}{\partial \alpha_{ab}} %&= \sum_{u,v} \mathcal{H}_{ab}(u,v) \langle y,u-v \rangle \nonumber\\
= \frac{1}{(N_1N_2)^2} \sum_{u,v} |H_a(u)||H_a(v)|\,[\varphi_b(u)-\varphi_b(v)]\,
\sin \left(\sum_j \alpha_{aj}[\varphi_j(u)-\varphi_j(v)] + %2\pi\frac{(u-v)y}{N_1N_2}\right)
  2\pi\frac{(u_1-v_1)y_1}{N_1}+ 2\pi\frac{(u_2-v_2)y_2}{N_2}\right)
\end{equation}
%We could have reached the same conclusion by noting that~(\ref{eq:m3}) must also be real.  That is,
%\begin{align*}
%\frac{\partial s_a(y)}{\partial \alpha_{ab}}  %&= \Real \frac{\partial s_a(y)}{\partial \alpha_{ab}} 
%&= \Real \sum_{u,v} \mathcal{H}_{ab}(u,v) \langle y,u-v \rangle \\
%&= \frac{1}{(N_1N_2)^2} \sum_{u,v} |H_a(u)||H_a(v)|\,[\varphi_b(u)-\varphi_b(v)]\,
%\cos \left(\frac{\pi}{2} + \sum_j \alpha_{aj}[\varphi_j(u)-\varphi_j(v)] + 2\pi \frac{(u-v)y}{N_1N_2} \right)
%\end{align*}

Substituting (\ref{eq:m8}) for $\sum_{u,v}\mathcal{H}_{ab}(u,v) \tau_{u-v}$ in
equation~(\ref{eq:m6}) yields
\begin{align}\label{eq:m9}
\frac{\partial \ell\left(\vec{g} |\vec{d}\right)}{\partial \alpha_{ab}}  
= \frac{1}{(N_1N_2)^2} \sum_{x,y}\sum_{u,v} 
      &\left[\frac{d_a(x)}{g_a(x)}-1\right]t(x)f(x-y)|H_a(u)||H_a(v)|\,[\varphi_b(u)-\varphi_b(v)] \\
      & \quad \times \sin \left(\sum_j \alpha_{aj}[\varphi_j(u)-\varphi_j(v)] + %2\pi \frac{(u-v)y}{N_1N_2}\right)\nonumber
  2\pi\frac{(u_1-v_1)y_1}{N_1}+ 2\pi\frac{(u_2-v_2)y_2}{N_2}\right)\nonumber
\end{align}
By (\ref{eq:m9}) we see that one way to find a parameter set 
$\{\alpha_{aj}\}_j$ yielding a zero gradient is to find one which satisfies
\begin{equation}\label{eq:m10}
\sum_{j=0}^{J-1} \alpha_{aj}[\varphi_j(u)-\varphi_j(v)] + %2\pi \frac{(u-v)y}{N_1N_2} = n\pi, \qquad n \in \Z
  2\pi\frac{(u_1-v_1)y_1}{N_1}+ 2\pi\frac{(u_2-v_2)y_2}{N_2} = n\pi, \qquad n \in \Z
\end{equation}

%%%
%%%Define %$\psi_j^k\in \C^{N^2-k}$
%%%$\psi_j^k$ to be a vector of length $N_1N_2-k$ with $m^{th}$ element given by 
%%%\[\psi_j^k(m) = \varphi_j(m+k)-\varphi_j(m), \qquad m \in \{0,1,\ldots,N_1N_2-k-1\}\]
%%%Also, define
%%%\[\alpha_a = \left(
%%%           \begin{array}{c} \alpha_{a0}\\\alpha_{a1}\\\vdots\\\alpha_{a(J-1)}\end{array}
%%%           \right), \qquad
%%%\Psi^k = \left(\psi_0^k\; \psi_1^k \; \cdots \; \psi_{J-1}^k\right)\]
%%%and let $\mathbf{1} = (1\,1\,\cdots\,1)^t$ represent the column vector of $N_1N_2-k$ unit elements.
%%%Then we can express condition (\ref{eq:m10}), for all $u,v$ such that $u-v = k$, as follows:
%%%\[
%%%\Psi^k\alpha_a = \sum_{j\in\groupJ}\alpha_{aj}\psi_j^k = \left(2\pi \frac{ky}{N_1N_2}- n\pi\right)\mathbf{1}, \qquad n \in \Z
%%%  2\pi\frac{(u_1-v_1)y_1}{N_1}+ 2\pi\frac{(u_2-v_2)y_2}{N_2} = n\pi, \qquad n \in \Z
%%%\]
%%%and we could then estimate the parameter set via normal equations.
%%%\begin{align}\label{eq:m11}
%%%(\Psi^k)^*\Psi^k\alpha_a &= \left(2\pi \frac{ky}{N_1N_2}- n\pi\right)(\Psi^k)^*\mathbf{1} \nonumber\\
%%%\Leftrightarrow \qquad \alpha_a &= \left(2\pi \frac{ky}{N_1N_2}- n\pi\right)[(\Psi^k)^*\Psi^k]^{-1}(\Psi^k)^*\mathbf{1}
%%%\end{align}
%%%The expression $(\Psi^k)^*\mathbf{1}$ is a telescoping sum of differences of
%%%basis functions elements, and the right hand side of (\ref{eq:m11}) probably
%%%simplifies quite a lot, though we have yet to write out the details to verify this.

\section{The Spatially Varying Case}\label{sec:spat-vary-case}
The foregoing assumes that the point spread function is spatially invariant.
We now state the problem for the more general spatially varying case.
Focusing at first on a single image frame, we need not involve the frame index $k$ 
until later.  

%Previously we worked with the $N_1 \times N_2$ indexing set using the group $A =
%\Z/N_1 \times \Z/N_2$ and its character group given by the standard presentation.
As before, identify $A = \Z/N_1 \times \Z/N_2$ with its character group by the
standard presentation.
\[
A = \Z/N_1 \times \Z/N_2 = 
(\Z/N_1 \times \Z/N_2)^* = A^*
\]

Let 
%    $x \in A$ %$x_0$ 
%be a spatial coordinate in the focal plane and let 
    $y \in A$ % $x_1$ 
be a spatial coordinate in the object plane.
Then we model the noiseless image as the superposition, 
\begin{equation}\label{eq:sv-g}
g = \sum_{y \in \groupA}f(y)s_y%, \qquad x\in \groupA
\end{equation}
where $g,\, f, \, s_y \in \LA$.  Note that in this model our point spread
function varies with $y$. 

Assume $s_y = h_y h_y^*$, where
\begin{equation}\label{eq:sv-h}
h_y = \frac{1}{N_1N_2} \invFT H_y = \frac{1}{N_1N_2} \sum_{u\in \groupA^*} H_y(u)\tau_u
\end{equation}  
At any point $x$ in the focal plane, (\ref{eq:sv-h}) evaluates to 
\[
h_y(x) = 
\frac{1}{N_1N_2} \sum_{u\in \groupA^*} H_y(u)\,\E^{\I 2\pi \frac{x_1u_1}{N_1}}\E^{\I 2\pi \frac{x_2u_2}{N_2}}, \qquad x \in \groupA
\]
%&=\frac{1}{N_1N_2} \sum_{u_1 = 0}^{N_1}\sum_{u_2=0}^{N_2} v^{x_1u_1 + x_2u_2}H_y(u_1,u_2),\qquadv=\E^{\I 2\pi \frac{x_1u_1}{N_1}}\E^{\I 2\pi \frac{x_2u_2}{N_2}}, \qquad x \in \groupA


Thus far, the equations defining the psf's and transfer function are identical
to the invariant case, with the frame index $k$ replaced by a spatial
coordinate in the object frame.  However, a basic difference arises in the
functional form of the coherent transfer function.
\begin{equation}\label{eq:sv-H}
H_y(u) = |H_y(u)|\,\E^{\I \Psi_y(u)}
\end{equation}
where 
\[
\Psi_y(u) = \sum_{\ell=0}^{L-1} \Phi_{\ell}(\beta_{\ell}y + (1-\beta_{\ell})u)
\]
The index $\ell$ represents locations along the elevation axis.  Thus the
total phase-aberration for the pair $u, \, y$ takes contributions 
from phase-aberration functions $\Phi_{\ell}$ at various altitudes, evaluated
at a point along the line connecting $u$ and $y$.

If we denote the set of basis functions by $\{\varphi_j\}_{0\leq j < J}$, then the
phase-aberration for a point $\ell$ along the elevation axis is represented by the
following expansion:
\[
%\varphi(u) = \sum_{j=1}^J \alpha_j \varphi_j(u) 
\Phi_{\ell} = \sum_{j=0}^{J-1} \alpha_{\ell j} \varphi_j
\]
The basis functions $\varphi_j$ usually do not depend on
$\ell$, whereas the coefficients $\alpha_{\ell j}$ must be
estimated for each $\ell \in \{0, 1, \ldots, L-1\}$.
Finally, we have 
\begin{equation}\label{eq:sv-abberation}
\Psi_y(u) = \sum_{\ell=0}^{L-1} \sum_{j=0}^{J-1} \alpha_{\ell j} \varphi_j(\beta_{\ell}y + (1-\beta_{\ell})u)
\end{equation}

\begin{remark}
During implementation, we should be able to exploit periodicities of the basis
functions.  This would facilitate, among other things, estimation of the coefficients
$\alpha_{\ell j}$ in equation~(\ref{eq:sv-abberation}).  In particular,
suppose $\varphi_j$ is $B$-periodic; that is,
\begin{equation}\label{eq:sv-period}
\varphi_j(x) = \varphi_j(x+y), \qquad y \in B
\end{equation}

If the order of $B$ divides $L$, say $M = L/o(B)$, then
\begin{equation}\label{eq:sv-coeff}
\sum_{\ell=0}^{L-1} \alpha_{\ell j} \varphi_j(\beta_{\ell}y +(1-\beta_{\ell})u)
= \sum_{m=0}^{M-1} \left( \sum_{y\in B}\alpha_{(m+y) j}\right) \varphi_j(\beta_m y+(1-\beta_m) u)
\end{equation}
We see that, without the benefit of periodicity, there are $L$ coefficients to compute.
Exploiting $B$-periodicity, we compute only $M = L/o(B)$ coefficients.

N.B.~the form of~(\ref{eq:sv-coeff}) is probably wrong due to the form
of the operand of $\varphi_j$.  To correct for this, we need to represent the
periodicities of $\varphi_j$ along the line connecting $y$ and $u$, rather than
in the simple form given by (\ref{eq:sv-period}).
\end{remark}

\subsection{Periodic PSF}
Since $s$ is ultimately a function of the basis $\{\varphi_j\}$, it should be possible
to exploit periodicities in $\varphi_j$ when working with $s$.
This would greatly facilitate algorithmic implementation
of the space-variant model.  
%It fact, such a method is necessary if the problem is to be tractable.

Suppose the group $A$ has order $N$.  Let $B$ be a subgroup of $A$ with order
$L = o(B)$, where $L = N/M$.  Later we address the problem of writing
$s_y$ as a periodic function.  For now, %write $s_y(x) = s(x,y)\in \vs{L}(A\times A)$ and 
assume $s_y$ is $B$-periodic in $y$; that is, for each $x\in A$,
\[
s_a(x) = s_{a + b}(x), \qquad a \in A, \, b\in B
\]
Then, by~(\ref{eq:sv-g}),
\begin{align*}
g &= \sum_{y \in \groupA}f(y)s_y\\
&= \sum_{a \in A/B}\sum_{b\in B}f(a+b)s_{a+b}\\
&= \sum_{a \in A/B}\left(\sum_{b\in B}f(a+b)\right)s_a
\end{align*}
This reduces the order of spatial variance to $o(A/B) = M$.

In order to find periodicities in $s_y$, consider its functional form.  By \ref{eq:sv-h}
\begin{align*}
s_y &=  h_y h_y^*\\
&= \frac{1}{(N_1N_2)^2} \left(\sum_{u\in \groupA^*} H_y(u)\tau_u\right)
\left(\sum_{v\in \groupA^*} H_y^*(v)\tau_v\right)\\
&=\frac{1}{(N_1N_2)^2}\sum_{u\in A^*} \sum_{v\in A^*} H_y(u)H_y^*(v)\tau_{u-v}
\end{align*}
Thus, we must locate periodicities in 
\[
H_y(u) = |H_y(u)|\,\E^{\I \Psi_y(u)}
\]
where 
\[
\Psi_y(u) = \sum_{\ell=0}^{L-1} \Phi_{\ell}(\beta_{\ell}y + (1-\beta_{\ell})u)
\]
%%%If not, a suitable application of a periodicity operator will make it so.

\subsection{Group Algebra Methods}
We now change gears and consider how the few tools from nonabelian group
theory that we described previously can be applied to the space-varying blur
problem. This subsection begins with a few definitions, some of which
merely re-iterate or emphasizes previously stated facts about groups and factor
groups.  Besides emphasizing important points, we aim to decrease the
dependence of this section on those preceeding it.

%Suppose the image $f$ is a real-valued function on the abelian group
Consider the abelian group $A = C_N(x) \times C_N(y)$.  Under the usual
identifications, we can write a typical point as $x^my^n, 0\leq m,n < N$,
subject to the relations $x^N = 1 = y^N$ and $xy = yx$.
\[
A = C_N(x) \times C_N(y) = \{x^m y^n : 0 \leq m,n < N \}
\]
where, as usual, $C_N(x) = \{x^m : 0 \leq m < N \}$ denotes a cyclic group of
order $N$ with generator $x$, and a binary multiply operator satisfying
\[
x^mx^n = x^{m+n \bmod N}, \qquad x^N=1
\]

Let $B$ be the subgroup of $A$ defined by
\[
B = C_M(x^L) \times C_M(y^L) = \{x^{pL} y^{qL} : 0 \leq p,q < M \}, \quad \text{ where } LM = N,
\]
Thus, $B$ is a direct product of cyclic groups of order $M$.
The \emph{factor group} $A/B$ is given by
\[
A/B = \{x^j y^k B : 0 \leq j,k < L \}
\]
Each element $x^j y^k B \in A/B$ is a direct product of cyclic subgroups of $A$.
The element $x^j y^k B$ is called the \emph{$B$-coset of $A$ with representative
$x^j y^k$}.  The elements of a particular coset are called \emph{equivalent in
$A$ modulo $B$}, or simply, \emph{equivalent modulo $B$}.
The complete set of \emph{$B$-coset representatives in $A$}  is 
\[
H = \{x^j y^k : 0 \leq j,k < L \} = C_L(x) \times C_L(y)
\]
Thus, $H$ is a direct product of cyclic groups of order $L$.
Furthermore, any element $a\in A$ can be uniquely written
\[
a = hb, \qquad h\in H, b\in B
\]
where $h$ specifies that $a$ belongs to the coset $hB$, and $b$ identifies $a$
within that coset. 
We give concrete examples of $B$-cosets for a few special cases.  
\begin{example}
First, let $N=8$, $M=2$ and $L=4$.  Then,
\begin{equation}\label{eq:I-9}
A = C_8(x) \times C_8(y) = \{x^m y^n : 0 \leq m,n < 8 \}
\end{equation}
and
\[
B = C_2(x^4) \times C_2(y^4) = \{x^{p4} y^{q4} : 0 \leq p,q < 2 \}
\]
In the following figure, the numbers denote exponents $m,n$ on the
elements $x^my^n \in A$ in (\ref{eq:I-9}). 
\[
\begin{matrix}  
\begin{array}{cc} & n\\
                m & \end{array} & 0 & 1 & 2 & 3 &  & 4 & 5 & 6 & 7\\
0 & \mathbf{00} & 01 & 02 & 03 & | & \mathbf{04} & 05 & 06 & 07\\
1 & 10 & 11 & 12 & 13 & | & 14 & 15 & 16 & 17\\
2 & 20 & 21 & 22 & 23 & | & 24 & 25 & 26 & 27\\
3 & 30 & 31 & 32 & 33 & | & 34 & 35 & 36 & 37\\
\hline
4 & \mathbf{40} & 41 & 42 & 43 & | & \mathbf{44} & 45 & 46 & 47\\
5 & 50 & 51 & 52 & 53 & | & 54 & 55 & 56 & 57\\
6 & 60 & 61 & 62 & 63 & | & 64 & 65 & 66 & 67\\
7 & 70 & 71 & 72 & 73 & | & 74 & 75 & 76 & 77
\end{matrix}\]
The boldface exponents comprise the $B$-coset with representative
$x^0y^0$; that is,
\[
x^0y^0B = B = \begin{bmatrix} 00 & 04 \\ 40 & 44 \end{bmatrix}
\]
The $B$-coset with representative $x^1y^0$ is 
\[
x^1y^0B = xB = \begin{bmatrix}10 & 14 \\ 50 & 54 \end{bmatrix}
\]
A few more examples are the sets
\[
yB = \begin{bmatrix} 01 & 05 \\ 41 & 45 \end{bmatrix}, \quad
xyB = \begin{bmatrix} 11 & 15 \\ 51 & 55 \end{bmatrix}
\]
\[
x^2B = \begin{bmatrix} 20 & 24 \\ 60 & 64 \end{bmatrix}, \quad
x^2yB = \begin{bmatrix} 21 & 25 \\ 61 & 65 \end{bmatrix}
\]
\end{example}
which are the $B$-cosets with representatives $x^0y^1$,
$x^1y^1$, $x^2y^0$, and $x^2y^1$, respectively.
%\begin{example}\[\begin{pmatrix}\mathbf{00} & 01 & 02 & 03 & \vdots & \mathbf{04} & 05 & 06 & 07\\
%10 & 11 & 12 & 13 & \vdots & 14 & 15 & 16 & 17\\
%20 & 21 & 22 & 23 & \vdots & 24 & 25 & 26 & 27\\
%30 & 31 & 32 & 33 & \vdots & 34 & 35 & 36 & 37\\
%& \hdotsfor{7}&\\
%\mathbf{40} & 41 & 42 & 43 & \vdots & \mathbf{44} & 45 & 46 & 47\\
%50 & 51 & 52 & 53 & \vdots & 54 & 55 & 56 & 57\\
%60 & 61 & 62 & 63 & \vdots & 64 & 65 & 66 & 67\\
%70 & 71 & 72 & 73 & \vdots & 74 & 75 & 76 & 77
%\end{pmatrix}\]\end{example}
%\[\begin{pmatrix}
%\mathbf{(0, 0)} & (0, 1) & (0, 2) & (0, 3) & \mathbf{(0, 4)} & (0, 5) & (0, 6) & (0, 7)\\
%(1, 0) & (1, 1) & (1, 2) & (1, 3) & (1, 4) & (1, 5) & (1, 6) & (1, 7)\\
%(2, 0) & (2, 1) & (2, 2) & (2, 3) & (2, 4) & (2, 5) & (2, 6) & (2, 7)\\
%(3, 0) & (3, 1) & (3, 2) & (3, 3) & (3, 4) & (3, 5) & (3, 6) & (3, 7)\\
%\mathbf{(4, 0)} & (4, 1) & (4, 2) & (4, 3) & \mathbf{(4, 4)} & (4, 5) & (4, 6) & (4, 7)\\
%(5, 0) & (5, 1) & (5, 2) & (5, 3) & (5, 4) & (5, 5) & (5, 6) & (5, 7)\\
%(6, 0) & (6, 1) & (6, 2) & (6, 3) & (6, 4) & (6, 5) & (6, 6) & (6, 7)\\
%(7, 0) & (7, 1) & (7, 2) & (7, 3) & (7, 4) & (7, 5) & (7, 6) & (7, 7)
%\end{pmatrix}\]

Define an \emph{action group} $K$ as follows:
\[
K = C_M(k_c)\times C_M(k_d) = \{k_c^p k_d^q : 0 \leq p,q < M\}, \quad
c = \begin{pmatrix} c_0 & c_1 \\ c_2 & c_3 \end{pmatrix}, \;
d = \begin{pmatrix} d_0 & d_1 \\ d_2 & d_3 \end{pmatrix}
\]
The semi-direct product $H\varangle K$ is the group with elements
\[
H\varangle K = \{x^j y^k k_c^p k_d^p : 0 \leq j,k < L, 0 \leq p, q < M\}
\]
and a binary multiplication operation satisfying the following:
\[
x^L = y^L = k_c^M = k_d^M = 1
\]
\[
x^{-1} = x^{L-1}, \quad y^{-1}= y^{L-1}, \quad k_c^{-1} = k_c^{M-1}, \quad k_d^{-1} = k_d^{M-1}
\]
\[
k_c x^j y^k = x^{c_0 j + c_1 k} y^{c_2 j + c_3 k} k_c, \quad
k_d x^j y^k = x^{d_0 j + d_1 k} y^{d_2 j + d_3 k} k_d
\]
where the summands in the exponents are modulo $N$.

\subsubsection{Object Model}
Assume the object of interest is $f\in \C G$, where $G = H\varangle K$.  That is
$f$ belongs to the group algebra and, as such,
\[ 
f = \sum_{a\in G} f(a) a = \sum_{j,k}\sum_{p,q} f(x^jy^kk_c^pk_d^q) x^jy^kk_c^pk_d^q
\]
We take the function $g\in \C H$ to be some imperfect representation of $f$.
For example, as a generalization of our current model, we might take $g$ to be a
blurry image formed by mixing the object with a point spread function $s$, which
we model as another element of $\C H$.  Then we would write the imperfect blurry
image as,
\[
g = fs = \sum_{j,k}\sum_{p,q} f(x^jy^kk_c^pk_d^q) \T(x^jy^kk_c^pk_d^q)s
\]
whereby the function $g$, when evaluated at the point $x^my^n$, is given by
\begin{equation}\label{eq:I-8}
g(x^my^n) = \sum_{j,k}\sum_{p,q} f(x^jy^kk_c^pk_d^q)
\T(k_c^pk_d^q)s(x^{m-j}y^{n-k})
\end{equation}
The important thing to take from equation~(\ref{eq:I-8}) -- and, in fact, the whole
point of applying group algebra methods in this context -- is that we have made
the translation operator more general.  When its operand is an element of the
abelian group $H$, the translation $\T$ is a simple shift operator,
\[
\T(x^jy^k)s(x^{m}y^{n}) =s(x^{m-j}y^{n-k})
\]
from which we arrive at the standard convolution product,
\[
(C(f)s)(x) = \sum_{y\in H} f(y)\T(y)s(x) = \sum_{y\in H} f(y)s(y^{-1}x)
\]
However, when its operand is an elements of the action group $K$,
a much richer class of translation operators are possible, and this class can be
constructed to include rotations, scale changes, and other revealing
transformations.  This is an important and powerful consideration since it
allows us to apply our existing fast algorithms for the standard convolution to
operations that are more general than simple spatial shifts.  

\subsection{Future Work for Anisoplanatism R \& D}
\paragraph{Overview.}  What follows is a brief, general outline of our
immediate concerns and objectives, afterwhich appears more specific details
describing how we plan to address our concerns and carry out our objectives.
\begin{enumerate}
\item {\bf Better synthesis of old and new theory}\\
The new theory is very general and, as yet, little has been done to
incorporate the finer details and special properties of our application.  
The old theory is very specialized and highly adapted to our application.  
We need to bridge this gap.
\item {\bf Experimentation, Simulation, Real Data}\\
Demostrate new capabilities and characterize performace using both simulated
and real data. 
\end{enumerate}

\paragraph{Immediate Objectives and Action Items.} By breaking down the items
in the foregoing section, we construct a list of more concrete goals, stated
in terms of observable actions with measureable results. 
\begin{enumerate}
\item {\bf Better synthesis of old and new theory}
\begin{enumerate}
\item Review the following references:
\begin{itemize}
\item Paul Billing's handwritten notes\label{item:revref1}
\item section~\ref{sec:poiss-object} above
\item section~\ref{sec:spat-vary-case} above
\end{itemize}
\item Consider special conditions (\eg assumptions made, constraints imposed)
  of the existing model as stated in Paul's notes and section~\ref{sec:poiss-object}.
\item Determine which of the special conditions are not reflected in the new paradigm
 and its corresponding model as described in section~\ref{sec:spat-vary-case}.
\item Consider how the special conditions could be incorporated into the new
  model and, if incorporated, what resulting improvements or advantages could
  we expect.
\end{enumerate}
\item {\bf Experimentation, Simulation, Real Data}
\begin{enumerate}
\item Develop Matlab prototypes for experimenting with and applying the new theory and methods. 
\begin{itemize}
\item Work with and learn from existing MFBD matlab code in cvs repository.
\item Work with and learn from matlab code acquired at NATO meeting.
\item Build upon and extend these assets by developing prototype code for
  implementing the model of section~\ref{sec:spat-vary-case} above. 
\end{itemize}
\item Experiment with various models of psf variation and consider what
  transformations provide the best model for anisoplanatism. 
\end{enumerate}
\end{enumerate}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "dspmath"
%%% End: 
